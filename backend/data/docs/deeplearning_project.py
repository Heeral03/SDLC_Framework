# -*- coding: utf-8 -*-
"""DeepLearning_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12_EoLxfCqY5HRNucwNuUInzVx5JEmzZJ
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import random
from PIL import Image
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader

# Path to your dataset
data_path = '/content/drive/MyDrive/DeepLearning_Project/Dataset_DL/'

# Transformations: resize + normalize + optional augmentation
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])
])

class SmallPaintingPhotoDataset(Dataset):
    def __init__(self, root, transform=None, n_per_class=50):
        self.transform = transform
        self.data = []
        self.labels = []
        classes = ['painting', 'photos']
        for idx, cls in enumerate(classes):
            cls_path = os.path.join(root, cls)
            all_images = os.listdir(cls_path)
            selected_images = random.sample(all_images, n_per_class)
            for img_name in selected_images:
                self.data.append(os.path.join(cls_path, img_name))
                self.labels.append(idx)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        img = Image.open(self.data[idx]).convert('RGB')
        if self.transform:
            img = self.transform(img)
        label = self.labels[idx]
        return img, label

dataset = '/content/drive/MyDrive/DeepLearning_Project/Dataset_DL/train'

# Training dataset: 50 + 50
train_dataset = SmallPaintingPhotoDataset(dataset, transform=transform, n_per_class=50)
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)

import matplotlib.pyplot as plt
import numpy as np

# Helper function to unnormalize and show image
def imshow(img, title=None):
    img = img / 2 + 0.5  # unnormalize from [-1,1] to [0,1]
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1,2,0)))  # C,H,W -> H,W,C
    if title:
        plt.title(title)
    plt.axis('off')
    plt.show()

# Get a batch of images
dataiter = iter(train_loader)
images, labels = next(dataiter)

# Class names
classes = ['Painting', 'Photos']

# Show first 8 images in the batch
for i in range(len(images)):
    imshow(images[i], title=classes[labels[i]])

valid_path = '/content/drive/MyDrive/DeepLearning_Project/Dataset_DL/valid'

valid_dataset = SmallPaintingPhotoDataset(valid_path, transform=transform, n_per_class=10)
valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False)

test_path = '/content/drive/MyDrive/DeepLearning_Project/Dataset_DL/test'

# Get all images
all_test_images = os.listdir(test_path)

# Pick random 20 images
random_images = random.sample(all_test_images, 20)

plt.figure(figsize=(12,6))
for i, img_name in enumerate(random_images):
    img_path = os.path.join(test_path, img_name)
    img = Image.open(img_path).convert('RGB')
    img_show = transforms.ToTensor()(img)  # just for display
    img_show = img_show / 2 + 0.5  # unnormalize for viewing

    plt.subplot(4,5,i+1)
    plt.imshow(img_show.permute(1,2,0))  # C,H,W -> H,W,C
    plt.title(img_name)
    plt.axis('off')
plt.tight_layout()
plt.show()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import os
import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# -------------------------
# Dataset
# -------------------------
class SmallPaintingPhotoDataset(Dataset):
    def __init__(self, root, transform=None, n_per_class=50):
        self.transform = transform
        self.data = []
        self.labels = []
        classes = ['painting', 'photos']
        for idx, cls in enumerate(classes):
            cls_path = os.path.join(root, cls)
            all_images = os.listdir(cls_path)
            selected_images = random.sample(all_images, n_per_class)
            for img_name in selected_images:
                self.data.append(os.path.join(cls_path, img_name))
                self.labels.append(idx)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        img = Image.open(self.data[idx]).convert('RGB')
        if self.transform:
            img = self.transform(img)
        label = self.labels[idx]
        return img, label

# -------------------------
# Transforms
# -------------------------
transform = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])
])

# -------------------------
# CNN Class (Global)
# -------------------------
class CNN(nn.Module):
    def __init__(self, mode='vanishing'):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(3,16,3,padding=1)
        self.conv2 = nn.Conv2d(16,32,3,padding=1)
        self.conv3 = nn.Conv2d(32,64,3,padding=1)
        self.pool = nn.MaxPool2d(2,2)
        self.fc1 = nn.Linear(64*28*28,128)
        self.fc2 = nn.Linear(128,2)

        self.act = nn.Tanh() if mode=='vanishing' else nn.ReLU()
        self.dropout = nn.Dropout(0.5)

    def forward(self,x):
        x = self.act(self.conv1(x))
        x = self.pool(x)
        x = self.act(self.conv2(x))
        x = self.pool(x)
        x = self.act(self.conv3(x))
        x = self.pool(x)
        x = x.view(-1,64*28*28)
        x = self.dropout(self.act(self.fc1(x)))
        x = self.fc2(x)
        return x

# -------------------------
# Gradient demonstration function
# -------------------------
def demonstrate_gradients(dataset_path, n_per_class=50, mode='vanishing'):
    dataset = SmallPaintingPhotoDataset(dataset_path, transform=transform, n_per_class=n_per_class)
    loader = DataLoader(dataset, batch_size=4, shuffle=True)

    model = CNN(mode=mode).to(device)

    # Exploding gradients: large weight init
    if mode=='exploding':
        for m in model.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight, gain=10)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # Take one batch
    images, labels = next(iter(loader))
    images, labels = images.to(device), labels.to(device)

    optimizer.zero_grad()
    outputs = model(images)
    loss = criterion(outputs, labels)
    loss.backward()

    # Print gradient norms
    print(f"--- Gradient norms ({mode}) ---")
    for name, param in model.named_parameters():
        if param.grad is not None:
            print(f"{name}: {param.grad.norm().item():.6f}")

# -------------------------
# Usage
# -------------------------
dataset_path = '/content/drive/MyDrive/DeepLearning_Project/Dataset_DL/train'

demonstrate_gradients(dataset_path, n_per_class=5, mode='vanishing')
demonstrate_gradients(dataset_path, n_per_class=5, mode='exploding')

def train_validate(model, train_loader, valid_loader, epochs=20, lr=0.001, weight_decay=1e-4, patience=3):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)  # L2 regularization
    train_loss_list, valid_acc_list = [], []

    best_acc = 0
    counter = 0

    for epoch in range(epochs):
        # Training
        model.train()
        running_loss = 0.0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
        train_loss_list.append(running_loss/len(train_loader))

        # Validation
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for images, labels in valid_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = torch.max(outputs,1)
                total += labels.size(0)
                correct += (predicted==labels).sum().item()
        acc = correct/total
        valid_acc_list.append(acc)

        print(f"Epoch {epoch+1}: Train Loss={running_loss/len(train_loader):.4f}, Valid Acc={acc:.2f}")

        # Early stopping
        if acc >  best_acc:
            best_acc = acc
            counter = 0
            torch.save(model.state_dict(), 'best_cnn_model.pth')
        else:
            counter += 1
            if counter >= patience:
                print("Early stopping triggered")
                break

    return train_loss_list, valid_acc_list

cnn_model = CNN(mode='vanishing').to(device)

# Train + validate
train_loss, valid_acc = train_validate(
    model=cnn_model,
    train_loader=train_loader,
    valid_loader=valid_loader,
    epochs=30,
    lr=0.001,
    weight_decay=1e-4,
    patience=7
)

import matplotlib.pyplot as plt

plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(train_loss, label='Train Loss')
plt.legend()
plt.subplot(1,2,2)
plt.plot(valid_acc, label='Validation Acc')
plt.legend()
plt.show()

classes = ['Painting','Photos']

cnn_model.eval()
predictions = []
for img_name in random_images:
    img_path = os.path.join(test_path, img_name)
    img = Image.open(img_path).convert('RGB')
    img_t = transform(img).unsqueeze(0).to(device)
    with torch.no_grad():
        output = cnn_model(img_t)
        _, pred = torch.max(output,1)
    predictions.append((img_name, classes[pred.item()]))

# Show results
for fname, pred_class in predictions:
    print(fname, "->", pred_class)

def evaluate_accuracy(model, data_loader):
    model.eval()  # set model to evaluation mode
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in data_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    acc = correct / total
    return acc

# Example usage:
final_valid_acc = evaluate_accuracy(cnn_model, valid_loader)
print(f"Final Validation Accuracy: {final_valid_acc*100:.2f}%")